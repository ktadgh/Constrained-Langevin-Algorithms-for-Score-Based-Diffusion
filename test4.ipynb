{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T05:50:23.654540600Z",
     "start_time": "2023-08-24T05:50:20.520045700Z"
    }
   },
   "id": "931206da4a80b08b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-24T05:52:57.787937600Z",
     "start_time": "2023-08-24T05:52:57.778934400Z"
    }
   },
   "outputs": [],
   "source": [
    "def G(gs):\n",
    "    '''\n",
    "    :param gs: a list of tensor functions\n",
    "    :return: a function sending a tensor to the stacked matrix of the functions of that tensor\n",
    "    '''\n",
    "    def G_gs(tensor):\n",
    "        x = torch.squeeze(tensor)\n",
    "        # print(\"Function input: \",tensor) # checking the input for debugging\n",
    "        # print(\"Function output:\" , torch.stack([g(tensor) for g in gs],0))\n",
    "        return torch.stack([g(x) for g in gs], 0)\n",
    "\n",
    "    return G_gs\n",
    "\n",
    "def J(gs, x):\n",
    "    '''Returns the Jacobian evaluated at x for a list gs of constraint functions'''\n",
    "    return torch.autograd.functional.jacobian(G(gs), torch.squeeze(x),create_graph=True, vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-78., grad_fn=<TraceBackward0>)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = [lambda x: x[0]**2 + x[1]**2,lambda x: x[0]**2 -x[1]**5 ]\n",
    "x = torch.tensor([1.,2.])\n",
    "x.requires_grad_()\n",
    "Jac= torch.trace(J(gs,x))\n",
    "Jac.requires_grad_()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T06:01:43.586469300Z",
     "start_time": "2023-08-24T06:01:43.571228500Z"
    }
   },
   "id": "b7ae58dc4d1a40bd"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([   2., -160.]),)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(outputs = Jac, inputs = x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T06:01:44.009617300Z",
     "start_time": "2023-08-24T06:01:44.004438200Z"
    }
   },
   "id": "9347eec60415b427"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of 'a': tensor([0.0000, 0.5000, 1.0000])\n",
      "Gradient of 'b': None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define tensors with requires_grad=True\n",
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([3.0, 2.0, 1.0], requires_grad=False)\n",
    "\n",
    "# Perform maximum operation\n",
    "result = torch.max(a, b)\n",
    "\n",
    "# Compute some loss based on the result\n",
    "loss = result.sum()\n",
    "\n",
    "# Perform backpropagation to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(\"Gradient of 'a':\", a.grad)\n",
    "print(\"Gradient of 'b':\", b.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-26T20:59:14.446596600Z",
     "start_time": "2023-08-26T20:59:14.436041700Z"
    }
   },
   "id": "c510287e62f2dceb"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def jacobian_trace(fn, x):\n",
    "    J = torch.autograd.functional.jacobian(fn, x,create_graph=True)\n",
    "    return torch.trace(J)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-26T23:25:23.055401400Z",
     "start_time": "2023-08-26T23:25:23.039456500Z"
    }
   },
   "id": "ca5a656d26a69717"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def jacobian_trace_vec(fn, x):\n",
    "    J = torch.autograd.functional.jacobian(fn, x, create_graph=True,vectorize =True)\n",
    "    return torch.trace(J)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-26T23:25:41.388003600Z",
     "start_time": "2023-08-26T23:25:41.329079900Z"
    }
   },
   "id": "5935e866eccca880"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x244e21d3610>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-26T23:29:28.006902800Z",
     "start_time": "2023-08-26T23:29:27.988313Z"
    }
   },
   "id": "b3163140a95c0162"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 1., 2.], requires_grad=True)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0.,1.,2.])\n",
    "x.requires_grad_()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-26T23:29:28.769624400Z",
     "start_time": "2023-08-26T23:29:28.724267100Z"
    }
   },
   "id": "1922674ee8b2b60b"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:298: UserWarning: Error detected in ReshapeAliasBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Tadgh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Tadgh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Tadgh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Tadgh\\AppData\\Local\\Temp\\ipykernel_4080\\2474408486.py\", line 1, in <module>\n",
      "    torch.autograd.gradcheck(jacobian_trace_vec, inputs = (lambda x: x**2,x))\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\gradcheck.py\", line 1476, in gradcheck\n",
      "    return _gradcheck_helper(**args)\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\gradcheck.py\", line 1485, in _gradcheck_helper\n",
      "    func_out = func(*tupled_inputs)\n",
      "  File \"C:\\Users\\Tadgh\\AppData\\Local\\Temp\\ipykernel_4080\\4055604990.py\", line 2, in jacobian_trace_vec\n",
      "    J = torch.autograd.functional.jacobian(fn, x, create_graph=True,vectorize =True)\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\functional.py\", line 646, in jacobian\n",
      "    flat_outputs = tuple(output.reshape(-1) for output in outputs)\n",
      "  File \"C:\\Users\\Tadgh\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\functional.py\", line 646, in <genexpr>\n",
      "    flat_outputs = tuple(output.reshape(-1) for output in outputs)\n",
      " (Triggered internally at ..\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:119.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Batching rule not implemented for aten::item. We could not generate a fallback.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[46], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradcheck\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjacobian_trace_vec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\gradcheck.py:1476\u001B[0m, in \u001B[0;36mgradcheck\u001B[1;34m(func, inputs, eps, atol, rtol, raise_exception, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode)\u001B[0m\n\u001B[0;32m   1474\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1475\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1476\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_gradcheck_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\gradcheck.py:1485\u001B[0m, in \u001B[0;36m_gradcheck_helper\u001B[1;34m(func, inputs, eps, atol, rtol, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode)\u001B[0m\n\u001B[0;32m   1482\u001B[0m tupled_inputs \u001B[38;5;241m=\u001B[39m _as_tuple(inputs)\n\u001B[0;32m   1483\u001B[0m _check_inputs(tupled_inputs, check_sparse_nnz)\n\u001B[1;32m-> 1485\u001B[0m func_out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtupled_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1486\u001B[0m outputs \u001B[38;5;241m=\u001B[39m _differentiable_outputs(func_out)\n\u001B[0;32m   1487\u001B[0m _check_outputs(outputs)\n",
      "Cell \u001B[1;32mIn[41], line 2\u001B[0m, in \u001B[0;36mjacobian_trace_vec\u001B[1;34m(fn, x)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mjacobian_trace_vec\u001B[39m(fn, x):\n\u001B[1;32m----> 2\u001B[0m     J \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjacobian\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mvectorize\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtrace(J)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\functional.py:657\u001B[0m, in \u001B[0;36mjacobian\u001B[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001B[0m\n\u001B[0;32m    654\u001B[0m         vj[el_idx] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros_like(inputs[el_idx])\u001B[38;5;241m.\u001B[39mexpand((\u001B[38;5;28msum\u001B[39m(output_numels),) \u001B[38;5;241m+\u001B[39m inputs[el_idx]\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m    655\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(vj)\n\u001B[1;32m--> 657\u001B[0m jacobians_of_flat_output \u001B[38;5;241m=\u001B[39m \u001B[43mvjp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    659\u001B[0m \u001B[38;5;66;03m# Step 3: The returned jacobian is one big tensor per input. In this step,\u001B[39;00m\n\u001B[0;32m    660\u001B[0m \u001B[38;5;66;03m# we split each Tensor by output.\u001B[39;00m\n\u001B[0;32m    661\u001B[0m jacobian_input_output \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\functional.py:650\u001B[0m, in \u001B[0;36mjacobian.<locals>.vjp\u001B[1;34m(grad_output)\u001B[0m\n\u001B[0;32m    649\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvjp\u001B[39m(grad_output):\n\u001B[1;32m--> 650\u001B[0m     vj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[43m_autograd_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflat_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[0;32m    651\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m el_idx, vj_el \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(vj):\n\u001B[0;32m    652\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m vj_el \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\functional.py:167\u001B[0m, in \u001B[0;36m_autograd_grad\u001B[1;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001B[0m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m,) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(inputs)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 167\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_grad_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_unused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_grads_batched\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:301\u001B[0m, in \u001B[0;36mgrad\u001B[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001B[0m\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvjp\u001B[39m(gO):\n\u001B[0;32m    298\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    299\u001B[0m             t_outputs, gO, retain_graph, create_graph, t_inputs,\n\u001B[0;32m    300\u001B[0m             allow_unused, accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m--> 301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_vmap_internals\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_vmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvjp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_none_pass_through\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad_outputs_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    304\u001B[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001B[0;32m    305\u001B[0m         allow_unused, accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\_vmap_internals.py:223\u001B[0m, in \u001B[0;36m_vmap.<locals>.wrapped\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    220\u001B[0m     batched_inputs, batch_size \u001B[38;5;241m=\u001B[39m _create_batched_inputs(\n\u001B[0;32m    221\u001B[0m         in_dims, args, vmap_level, func\n\u001B[0;32m    222\u001B[0m     )\n\u001B[1;32m--> 223\u001B[0m     batched_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatched_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    224\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_none_pass_through:\n\u001B[0;32m    225\u001B[0m         _validate_outputs(batched_outputs, func)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Thesis\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:298\u001B[0m, in \u001B[0;36mgrad.<locals>.vjp\u001B[1;34m(gO)\u001B[0m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvjp\u001B[39m(gO):\n\u001B[1;32m--> 298\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    299\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgO\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    300\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_unused\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Batching rule not implemented for aten::item. We could not generate a fallback."
     ]
    }
   ],
   "source": [
    "torch.autograd.gradcheck(jacobian_trace_vec, inputs = (lambda x: x**2,x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-26T23:29:30.730269100Z",
     "start_time": "2023-08-26T23:29:29.369236600Z"
    }
   },
   "id": "57f1dbd2e50f8b10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "48a8314db21613d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
