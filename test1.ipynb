{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# Detach the tensor 'x'\n",
    "x_detached = x.detach()\n",
    "\n",
    "# Calculate the Jacobian of 'y' with respect to 'x'\n",
    "jac = torch.autograd.functional.jacobian(lambda x: y+x, x_detached)\n",
    "\n",
    "print(jac)  # Output: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    y = x.detach().numpy()\n",
    "    z = y**2 + y*5\n",
    "    return torch.tensor(z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.jacobian(function, torch.tensor([2.]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    z = torch.tensor([0.], requires_grad=True)\n",
    "    for i in range(5):\n",
    "        z += 2*x\n",
    "    return z"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjacobian\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2.\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Users\\Desktop\\Thesis\\venv\\lib\\site-packages\\torch\\autograd\\functional.py:591\u001B[0m, in \u001B[0;36mjacobian\u001B[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001B[0m\n\u001B[0;32m    588\u001B[0m is_inputs_tuple, inputs \u001B[38;5;241m=\u001B[39m _as_tuple(inputs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minputs\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjacobian\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    589\u001B[0m inputs \u001B[38;5;241m=\u001B[39m _grad_preprocess(inputs, create_graph\u001B[38;5;241m=\u001B[39mcreate_graph, need_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 591\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    592\u001B[0m is_outputs_tuple, outputs \u001B[38;5;241m=\u001B[39m _as_tuple(outputs,\n\u001B[0;32m    593\u001B[0m                                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutputs of the user-provided function\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    594\u001B[0m                                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjacobian\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    595\u001B[0m _check_requires_grad(outputs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutputs\u001B[39m\u001B[38;5;124m\"\u001B[39m, strict\u001B[38;5;241m=\u001B[39mstrict)\n",
      "Cell \u001B[1;32mIn[32], line 4\u001B[0m, in \u001B[0;36mfunction\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      2\u001B[0m z \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m0.\u001B[39m], requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m----> 4\u001B[0m     z \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39mx\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m z\n",
      "\u001B[1;31mRuntimeError\u001B[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "torch.autograd.functional.jacobian(function, torch.tensor([2.]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "image= torch.zeros((39,2), dtype = torch.double)\n",
    "avg_pose = torch.zeros(39*3, dtype = torch.double)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "eigenposes = torch.ones((39*3,39*3), dtype = torch.double)\n",
    "eigenvalues= torch.ones(39*3, dtype = torch.double)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def neg_log_density(pose2):\n",
    "    pose = pose2.reshape((39,3))\n",
    "\n",
    "    transform = torch.zeros((39,3))\n",
    "    for i in range(39):\n",
    "        transform[i][2] = 1\n",
    "\n",
    "    pose = torch.squeeze(pose) + transform\n",
    "    fx = 1.0\n",
    "    fy = 1.0\n",
    "    cx = 0\n",
    "    cy = 0\n",
    "    a1 = torch.tensor([[fx], [0], [cx]], dtype=torch.double)\n",
    "    a2 = torch.tensor([[0], [fy], [cy]], dtype=torch.double)\n",
    "    a3 = torch.tensor([[0],[0],[1]], dtype=torch.double)\n",
    "\n",
    "    # Map the 3D point to 2D point\n",
    "    p1 = pose@ a1\n",
    "    p2 = pose @ a2\n",
    "    p3 = pose @ a3\n",
    "\n",
    "    projection = torch.hstack([p1/p3, p2/p3])\n",
    "\n",
    "    n = projection - image\n",
    "    #print(n)\n",
    "    # note - taking the variance to be 1...\n",
    "    pi = torch.tensor([0.], requires_grad=True)\n",
    "    # for point in n:\n",
    "    #     pi = pi+ torch.linalg.norm(point)**2 / 66.36\n",
    "    #\n",
    "    for i in range(len(eigenvalues)):\n",
    "        pi = pi+ torch.dot(torch.tensor(eigenposes[i]),(torch.flatten(pose) -torch.tensor(avg_pose)))**2\n",
    "    return pi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "x = torch.ones((39*3), dtype = torch.double)*3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n        3., 3., 3., 3., 3., 3., 3., 3., 3.], dtype=torch.float64)"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6744\\2535238615.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pi = pi+ torch.dot(torch.tensor(eigenposes[i]),(torch.flatten(pose) -torch.tensor(avg_pose)))**2\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.,\n         91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260., 91260.]],\n       dtype=torch.float64)"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.jacobian(neg_log_density, x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
